{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings from training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    im_embeddings = embeddings[:, :int(embeddings.shape[1] / 2)]\n",
    "    #im_embeggings_np = im_embeddings.numpy()\n",
    "    anchor_emb = embeddings[:, int(embeddings.shape[1] / 2):]\n",
    "    anchor_emb = tf.expand_dims(anchor_emb[0], axis=0)\n",
    "    #anchor_emb_np = anchor_emb.numpy()\n",
    "    dot_product = tf.matmul(im_embeddings, tf.transpose(anchor_emb))\n",
    "    #dot_product_np = dot_product.numpy()\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm_a = tf.reduce_sum(tf.square(im_embeddings), axis=1, keepdims=True)\n",
    "    #square_norm_a_np = square_norm_a.numpy()\n",
    "    square_norm_b = tf.reduce_sum(tf.square(anchor_emb), axis=1, keepdims=True)\n",
    "    #square_norm_b_np = square_norm_b.numpy()\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.add(square_norm_a, square_norm_b - 2.0*dot_product)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.cast(tf.equal(distances, 0.0), float)\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "    #distances_np = distances.numpy()\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    multiple = tf.constant([1, tf.shape(labels)[0].numpy()], tf.int32)\n",
    "    positive = tf.tile(labels, multiple)\n",
    "    positive = tf.cast(positive, tf.bool)\n",
    "    positive_np = positive.numpy()\n",
    "\n",
    "    return positive\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    multiple = tf.constant([1, tf.shape(labels)[0].numpy()], tf.int32)\n",
    "    positive = tf.tile(labels, multiple)\n",
    "    positive = tf.cast(positive, tf.bool)\n",
    "    negative = tf.logical_not(positive)\n",
    "    positive_np = negative.numpy()\n",
    "\n",
    "    return negative\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(y_true, y_pred):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the pairwise distance matrix\n",
    "    \n",
    "    #margin = 1.\n",
    "    labels = y_true\n",
    "    squared=False\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "    #label_np = labels.numpy()\n",
    "    embeddings = y_pred # shape 2048 (embeddings of the image + anchor)\n",
    "\n",
    "    # save embeddings to file\n",
    "    \n",
    "\n",
    "    # Compute the 2D matrix of distances between all the embeddings.\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    '''\n",
    "    #pairwise_dist_np = pairwise_dist.numpy()\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    #mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    #mask_anchor_positive = tf.cast(mask_anchor_positive, float)\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(tf.cast(labels, float), pairwise_dist)\n",
    "    #anchor_positive_dist_np=anchor_positive_dist.numpy()\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=0)\n",
    "    '''\n",
    "\n",
    "    # Calculate all the distances between the anchor and all the positives (same label as anchor)\n",
    "    anchor_positive_dist = tf.multiply(tf.cast(labels, float), pairwise_dist)\n",
    "    # Find the HARDEST TO GUESS POSITIVE, which is the one that is the FARTHEST away => largest distance\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=0)\n",
    "\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    '''\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    #mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    #mask_anchor_negative = tf.cast(mask_anchor_negative, float)\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    #max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    '''\n",
    "    # Calculate all the distances between the anchor and all the negatives (same label as anchor)\n",
    "    anchor_negative_dist = tf.multiply(1-(tf.cast(labels, float)), pairwise_dist)\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    # Get the negative samples distance where the distance is not 0\n",
    "    where = tf.not_equal(anchor_negative_dist, zero)\n",
    "    # Find the HARDEST TO GUESS NEGATIVE, which is the one that is CLOSEST to the anchor => min value\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist[where], axis=0)\n",
    "\n",
    "    # Find the difference in distance between the Hardest Positive and the Hardest negative!\n",
    "    D = hardest_positive_dist - hardest_negative_dist\n",
    "    # Calculate the margin given the formula in the thesis\n",
    "    margin = tf.math.log(1 + tf.math.exp(D))\n",
    "    \n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "\n",
    "    #triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss\n",
    "\n",
    "# Find the Accuracy based on the hardest positive and hardest negative\n",
    "def compute_accuracy_hard(y_true, y_pred):\n",
    "    #del y_true\n",
    "\n",
    "    labels = y_true\n",
    "    squared = False\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "    margin = 1.\n",
    "    embeddings = y_pred\n",
    "\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # Find Hardest Positive\n",
    "    anchor_positive_dist = tf.multiply(tf.cast(labels, float), pairwise_dist)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=0)\n",
    "    # Find positives that don't have the distance 0\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    where = tf.not_equal(anchor_positive_dist, zero)\n",
    "    positive_non_zero = anchor_positive_dist[where]\n",
    "    # shape (batch_size, 1)\n",
    "\n",
    "    # Find hardest negatives\n",
    "    # For each anchor, get the hardest negative\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    anchor_negative_dist = tf.multiply(1 - (tf.cast(labels, float)), pairwise_dist)\n",
    "    # shape (batch_size,)\n",
    "    # Find negatives that don't have the distance 0\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    where = tf.not_equal(anchor_negative_dist, zero)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist[where], axis=0)\n",
    "\n",
    "    # Calculate the difference in distance and the margin\n",
    "    D = hardest_positive_dist - hardest_negative_dist\n",
    "    margin = tf.math.log(1 + tf.math.exp(D))\n",
    "\n",
    "    # Selects positives that don't have distance 0, but smaller than the hardest negative distance minus margin\n",
    "    positive_less_negative = tf.less_equal(positive_non_zero, hardest_negative_dist - margin)\n",
    "    positive_less_negative = tf.cast(positive_less_negative, float)\n",
    "    \n",
    "    # Calculate the average value of those distances\n",
    "    # TODO: Question - why is this a metric of accuracy?\n",
    "    accuracy = tf.reduce_mean(positive_less_negative)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def background_thumbnail(template, modality, thumbnail_size=(200,200)):\n",
    "    foreground = Image.fromarray(template).convert(modality)\n",
    "    background = Image.new(modality, thumbnail_size, \"white\")\n",
    "    foreground.thumbnail(thumbnail_size)\n",
    "    (w, h) = foreground.size\n",
    "    \n",
    "    upper_left=(int((thumbnail_size[0] - w) / 2), int((thumbnail_size[1] - h) / 2))\n",
    "    background.paste(foreground, upper_left)\n",
    "    return np.array(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_score(file_name):\n",
    "    split = file_name.split('_')\n",
    "    if (split[1].lower() == '1.png'):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = ['cross', 'face', 'rail', 'rombo', 'rett_diag', 'rect', 'cross_vert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "root = '../'\n",
    "input_shape = (100, 100, 3)\n",
    "models_folder = root + 'first_model_old_3'\n",
    "\n",
    "template_dir = os.path.join(root, 'templates')\n",
    "\n",
    "for p in range(len(patterns)):\n",
    "    # if p < 4:\n",
    "    #     continue\n",
    "    \n",
    "    im_template = cv2.imread(os.path.join(template_dir, patterns[p] + \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "    im_template = background_thumbnail(im_template, 'L', (input_shape[0], input_shape[1]))\n",
    "    im_template = im_template.astype('float32')\n",
    "    im_template /= 255.\n",
    "    im_template = np.repeat(im_template[...,np.newaxis], 3, -1)\n",
    "\n",
    "    model = load_model(models_folder + '/best_model_triplet_' + patterns[p] + '_transfer.hdf5', \n",
    "                                custom_objects={'batch_hard_triplet_loss': batch_hard_triplet_loss,\n",
    "                                                'compute_accuracy_hard': compute_accuracy_hard})\n",
    "\n",
    "    images = os.listdir(os.path.join(root, 'cropped_completi', str(p))) \n",
    "\n",
    "    anchors_dataset = [im_template for i in range(len(images)) ]\n",
    "    images_dataset = []\n",
    "    labels_dataset = []\n",
    "\n",
    "    for img in images:\n",
    "        image_matrix = cv2.imread(os.path.join(root, 'cropped_completi', str(p), img), cv2.IMREAD_GRAYSCALE)\n",
    "        image_padded = background_thumbnail(image_matrix, 'L', (input_shape[0], input_shape[1]))\n",
    "        image_padded = image_padded.astype('float32')\n",
    "        image_padded /= 255.\n",
    "        image_padded = np.repeat(image_padded[...,np.newaxis], 3, -1)\n",
    "\n",
    "        images_dataset.append(image_padded)\n",
    "        labels_dataset.append(find_score(img))\n",
    "\n",
    "    images_dataset = np.array(images_dataset)\n",
    "    anchors_dataset= np.array(anchors_dataset)\n",
    "    embeddings = model.predict([images_dataset, anchors_dataset], batch_size=32)\n",
    "\n",
    "    distances = _pairwise_distances(embeddings, squared=False).numpy().reshape(-1)\n",
    "\n",
    "    embeddings_dataframe = pd.DataFrame(columns=['name', 'scores', 'labels', 'embeddings'])\n",
    "\n",
    "    embeddings2 = [json.dumps(e.tolist()) for e in list(embeddings)]\n",
    "    images2 = [i.split('_')[0] for i in images]\n",
    "\n",
    "    embeddings_dataframe['name'] = images2\n",
    "    embeddings_dataframe['scores'] = distances\n",
    "    embeddings_dataframe['labels'] = labels_dataset\n",
    "    embeddings_dataframe['embeddings'] = embeddings2\n",
    "\n",
    "    embeddings_dataframe.to_csv('../results/embeddings_' + str(p) + '.csv', header = True, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d073f89a8355d11ddac5a448b200c1c879fa5680801c761ed6fcc477c84e1a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('rocf2_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
